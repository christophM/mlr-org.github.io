---
layout: post
title: Stepwise Bayesian Optimization with mlrMBO
author: jakob
draft: false
---
  
```{r setup, include = FALSE, cache = FALSE, message = FALSE}
library(knitr)
knitr::opts_chunk$set(cache = FALSE, collapse = FALSE, fig.height = 4)
knitr::knit_hooks$set(document = function(x){
  gsub("```\n*```r*\n*", "", x)
})
library(mlr)
library(mlrMBO)
library(ggplot2)
library(rgenoud)
```

With the release of the new version of [mlrMBO](https://mlr-org.github.io/mlrMBO/) we added some minor fixes and added a nice *[Human-in-the-loop](https://mlr-org.github.io/mlrMBO/articles/supplementary/human_in_the_loop_MBO.html)* feature.
It enables you to sequentially

* visualize the state of the surrogate model,
* get the suggested parameter configuration for the next iteration and
* update the surrogate model with arbitrary evaluations.

In the following we will demonstrate this feature on a simple example.
<!--more-->

First we need something we want to optimize.
For this post a simple function will suffice but note that this function could also be an external process as in this mode **mlrMBO** does not need to access the objective function as you will do all the interaction.
```{r function}
library(mlrMBO)
set.seed(1)

fun = function(x) {
  x^2 + sin(2 * pi * x) * cos(0.3 * pi * x)
}
```

However we still need to define the range of our search space.
```{r parmset}
ps = makeParamSet(
  makeNumericParam("x", lower = -3, upper = 3)
)
```

And we also need some initial values to start the optimization.
```{r design}
des = generateDesign(n = 3, par.set = ps)
des$y = apply(des, 1, fun)
des
```

With these values we can initialize our sequential MBO object.
```{r control}
ctrl = makeMBOControl()
ctrl = setMBOControlInfill(ctrl, crit = crit.ei)
opt.state = initSMBO(par.set = ps, design = des, control = ctrl, minimize = TRUE, noisy = FALSE)
```

The `opt.state` now object contains all necessary information for the optimization.
We can even plot it to see how the Gaussian process models the objective function.
```{r optstate1}
plot(opt.state)
```

In the first panel the *expected improvement* ($EI = E(y_{min}-\hat{y})$) (see [Jones et.al.](http://www.ressources-actuarielles.net/EXT/ISFA/1226.nsf/0/f84f7ac703bf5862c12576d8002f5259/$FILE/Jones98.pdf)) is plotted over the search space.
The maximum of the *EI* is the point that we should evaluate next.
The second panel shows the mean prediction of the surrogate model, which is the Gaussian regression model aka *Kriging*.
The third panel shows the uncertainty prediction of the surrogate.
We can see, that the *EI* is high at points, where the mean prediction is low and/or the uncertainty is high.

To obtain the specific configuration suggested by mlrMBO for the next evaluation of the objective we can run:
```{r suggest1}
prop = proposePoints(opt.state)
prop
```

We will execute our objective function with the suggested value for `x` and feed it back to mlrMBO:
```{r eval1}
y = fun(prop$prop.points$x)
y
updateSMBO(opt.state, x = prop$prop.points, y = y)
```

We can automate this easily:
```{r evalloop, echo=FALSE}
replicate(3, {
  prop = proposePoints(opt.state)
  y = fun(prop$prop.points$x)
  updateSMBO(opt.state, x = prop$prop.points, y = y)
})
```

Let's see how the surrogate models the true objective function after having seen seven configurations:
```{r optstate2}
plot(opt.state, scale.panels = TRUE)
```

The nice thing about the *human-in-the-loop* mode is, that you don't have to stick to the suggestion.
In other words we can feed the surrogate with values without receiving a proposal.
Let's assume we have an expert who tells us to evaluate the values $x=-2$ and $x=2$ we can easily do so:
```{r feedmanual}
custom.prop = data.frame(x = c(-2,2))
ys = apply(custom.prop, 1, fun)
updateSMBO(opt.state, x = custom.prop, y = as.list(ys))
plot(opt.state, scale.panels = TRUE)
```

You can convert the `opt.state` object from this run to a normal `mlrMBO` result object like this:
```{r finalize}
res = finalizeSMBO(opt.state)
res
```

For the curious, let's see how our original function actually looks like:
```{r plottrue}
plot(fun, -3, 3)
points(x = getOptPathX(res$opt.path)$x, y = getOptPathY(res$opt.path))
```

So we see that we unfortunately didn't explore the global minimum, because we listened to the *"expert"*.
If look at the *EI* from the previous plot we see that MBO would suggests to evaluate the function between $0$ and $0.5$.

For more in-depth information look at the [Vignette for Human-in-the-loop MBO](https://mlr-org.github.io/mlrMBO/articles/supplementary/human_in_the_loop_MBO.html) and check out the other topics of our [page](https://mlr-org.github.io/mlrMBO).






